# XAI Research

## Objective

The goal of my research on **Explainable AI (XAI)** was to explore potential uses of XAI methods and assess their performance. This involved:

1. Conducting a **literature review** analyzing the **pros and cons** of XAI and evaluating its performance.
2. Performing **hands-on research** using the **Boston Housing Dataset**, where I trained a **Random Forest Regression Model** to predict housing prices based on neighborhood and housing characteristics.

## Research Methodology

For the experimental portion, I employed two prominent XAI techniques to interpret the predictions made by the model:

- **LIME (Local Interpretable Model-agnostic Explanations)**
- **SHAP (SHapley Additive exPlanations)**

These methods allowed me to generate both **local** and **global explanations** of the modelâ€™s behavior. Through experimentation, I compared the effectiveness and interpretability of these two methods, noting their respective strengths and limitations:

- **SHAP**: Known for **consistency** and **theoretical soundness**.
- **LIME**: Offers more **flexibility**, but can be computationally intensive.

## Key Findings

- **LIME** and **SHAP** are both valuable tools for model interpretation.
- **SHAP** excels in explaining the global behavior of the model with consistent results, while **LIME** is more flexible for local interpretation but may require more computational resources.
  
This experimentation helped reinforce the findings from my literature review and demonstrated the importance of **explainability** in building trustworthy, transparent AI systems.

## Cited Papers

- Angelov, Plamen P., et al. "Explainable Artificial Intelligence: An Analytical Review." *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, vol. 11, no. 5, 2021, Art. ID e1424. DOI: [10.1002/widm.1424](https://doi.org/10.1002/widm.1424)
  
- Dwivedi, Ru
