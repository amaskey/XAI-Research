# -*- coding: utf-8 -*-
"""BostonDatasetLIME/SHAPDemo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nmvCshLxgKVofe2VfB9ePTsW3VORt072

# Task
Write code that uses a machine learning model to analyze the dataset and use LIME to explain it.

Here is all the data you need:
"HousingData.csv"

## Data loading

### Subtask:
Load the housing data from the "HousingData.csv" file.

**Reasoning**:
Load the housing data from the provided CSV file into a pandas DataFrame and display the first few rows to verify the data has been loaded correctly.
"""

import pandas as pd

try:
    df = pd.read_csv('HousingData.csv')
    display(df.head())
except FileNotFoundError:
    print("Error: 'HousingData.csv' not found. Please ensure the file exists in the current directory.")
    df = None  # Indicate that the DataFrame could not be created
except pd.errors.ParserError:
    print("Error: Could not parse the CSV file. Please check the file format.")
    df = None

"""## Data exploration

### Subtask:
Explore the loaded dataset to understand its characteristics.

**Reasoning**:
Explore the dataset by examining its shape, data types, descriptive statistics, and identify missing values.
"""

# Examine the data's shape
print("Shape of the DataFrame:", df.shape)

# Inspect data types
print("\nData Types:\n", df.dtypes)

# Summarize numerical features
print("\nDescriptive Statistics:\n", df.describe())

# Identify missing values
print("\nMissing Values:\n", df.isnull().sum())

"""**Reasoning**:
Analyze categorical features, visualize data distributions, and calculate correlations to further understand the dataset.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Analyze categorical features (CHAS in this case, as it's the only one that seems categorical)
print("\nUnique values and counts for CHAS:\n", df['CHAS'].value_counts())

# Visualize data distributions
plt.figure(figsize=(15, 10))

# Histograms for numerical features
for i, col in enumerate(df.select_dtypes(include=['float64', 'int64']).columns):
    plt.subplot(4, 4, i + 1)
    plt.hist(df[col].dropna(), bins=20)  # Handle potential missing values in histograms
    plt.title(col)
plt.tight_layout()
plt.show()


# Bar charts for the categorical feature (CHAS)
plt.figure(figsize=(6, 4))
df['CHAS'].value_counts().plot(kind='bar')
plt.title('CHAS Distribution')
plt.show()

# Calculate and visualize correlations
correlation_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""## Data cleaning

### Subtask:
Clean the data by handling missing values and outliers.

**Reasoning**:
Impute missing values using the median for numerical features and the most frequent category for the categorical feature 'CHAS'. Then, cap outliers at the 1st and 99th percentiles for numerical features, and finally convert 'CHAS' to an integer type.
"""

# Impute missing values
for col in ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'LSTAT']:
    df[col] = df[col].fillna(df[col].median())
df['CHAS'] = df['CHAS'].fillna(df['CHAS'].mode()[0])

# Cap outliers
for col in ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']:
    lower_bound = df[col].quantile(0.01)
    upper_bound = df[col].quantile(0.99)
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

# Convert CHAS to integer
df['CHAS'] = df['CHAS'].astype(int)

display(df.head())

"""## Feature engineering

### Subtask:
Engineer new features and scale numerical features to improve model performance.

**Reasoning**:
Create interaction features and scale numerical features using StandardScaler.
"""

import numpy as np
from sklearn.preprocessing import StandardScaler

# Create interaction features
df['RM_LSTAT_interaction'] = df['RM'] * df['LSTAT']
df['CRIM_NOX_interaction'] = df['CRIM'] * df['NOX']


# Scale numerical features
numerical_cols = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'RM_LSTAT_interaction', 'CRIM_NOX_interaction']
scaler = StandardScaler()

# Fit the scaler on the numerical features
scaler.fit(df[numerical_cols])

# Transform the numerical features using the fitted scaler
df[numerical_cols] = scaler.transform(df[numerical_cols])

display(df.head())

"""## Data splitting

### Subtask:
Split the data into training and testing sets.

**Reasoning**:
Split the data into training and testing sets using `train_test_split`, stratifying by the `CHAS` column.
"""

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = df.drop('MEDV', axis=1)
y = df['MEDV']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df['CHAS'])

"""## Model training

### Subtask:
Train a RandomForestRegressor model on the training data.

**Reasoning**:
Train a RandomForestRegressor model on the training data.
"""

from sklearn.ensemble import RandomForestRegressor

# Initialize and train the RandomForestRegressor model
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)

"""## Model evaluation

### Subtask:
Evaluate the trained RandomForestRegressor model's performance on the test set.

**Reasoning**:
Evaluate the trained RandomForestRegressor model's performance on the test set using the provided test features (X_test) and target variables (y_test). Calculate MAE, MSE, RMSE, and R-squared.
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

"""## Model optimization

### Subtask:
Optimize the RandomForestRegressor model using RandomizedSearchCV.

**Reasoning**:
Optimize the RandomForestRegressor model using RandomizedSearchCV, print the best hyperparameters, create a new model with these hyperparameters, and train it.
"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import randint, uniform

# Define the parameter grid
param_grid = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(10, 50),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.1, 0.9),  # Use uniform distribution for max_features
    'bootstrap': [True, False]
}


# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_distributions=param_grid,
    n_iter=100,  # Number of parameter settings sampled
    cv=5,        # Number of cross-validation folds
    scoring='neg_mean_squared_error',
    random_state=42,
    n_jobs=-1    # Use all available cores
)

# Fit the randomized search to the training data
random_search.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters:", random_search.best_params_)

# Create a new RandomForestRegressor with the best hyperparameters
best_rf_model = RandomForestRegressor(**random_search.best_params_, random_state=42)

# Train the optimized model on the training set
best_rf_model.fit(X_train, y_train)

"""## Data preparation

### Subtask:
Prepare a small subset of the test data for LIME explanations.

**Reasoning**:
Create a small subset of the test data and corresponding target variables for LIME explanations.
"""

import pandas as pd

# Create a DataFrame for LIME explanations
df_lime = X_test.sample(n=10, random_state=42)
df_lime['MEDV'] = y_test[df_lime.index]

# Display the first few rows of df_lime
display(df_lime.head())

"""## Model evaluation

### Subtask:
Use LIME to explain the predictions of the optimized model (`best_rf_model`) on the prepared data subset (`df_lime`). Visualize the explanations.

**Reasoning**:
Import necessary libraries and create a LimeTabularExplainer object. Then, iterate through the rows of df_lime, generate explanations for each row, and visualize them.
"""

! pip install lime
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt

# Create a LimeTabularExplainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=None,
    categorical_features=[3],  # Assuming 'CHAS' is categorical
    mode='regression'
)

# Iterate through rows of df_lime and generate explanations
for index, row in df_lime.iterrows():
    exp = explainer.explain_instance(
        data_row=np.array(row.drop('MEDV')),
        predict_fn=best_rf_model.predict,
        num_features=len(X_train.columns)
    )

    # Visualize the explanation
    exp.as_pyplot_figure()
    plt.title(f"LIME Explanation for Instance {index} (Actual MEDV: {row['MEDV']})")
    plt.show()

!pip install lime

import lime
import lime.lime_tabular
import matplotlib.pyplot as plt
import numpy as np

# Create a LimeTabularExplainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=None,
    categorical_features=[3],  # Assuming 'CHAS' is categorical
    mode='regression'
)

# Iterate through rows of df_lime and generate explanations
for index, row in df_lime.iterrows():
    exp = explainer.explain_instance(
        data_row=np.array(row.drop('MEDV')),
        predict_fn=best_rf_model.predict,
        num_features=len(X_train.columns)
    )

    # Visualize the explanation
    exp.as_pyplot_figure()
    plt.title(f"LIME Explanation for Instance {index} (Actual MEDV: {row['MEDV']})")
    plt.show()

"""**Reasoning**:
The code executed, but there were warnings about feature names.  I need to ensure that the `best_rf_model` was trained with feature names, and that the input data for LIME also has the same feature names. The previous code block did not include handling of this situation, so I will fix the code.


"""

import lime
import lime.lime_tabular
import matplotlib.pyplot as plt
import numpy as np

# Assuming X_train has valid column names, and best_rf_model was trained with these names
# Create a LimeTabularExplainer, ensuring feature names are consistent
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=None,
    categorical_features=[X_train.columns.get_loc('CHAS')], # Use get_loc to find the index
    mode='regression'
)

# Iterate through df_lime and generate explanations
for index, row in df_lime.iterrows():
  # Ensure the input data to predict_fn also has valid feature names
    exp = explainer.explain_instance(
        data_row=np.array(row.drop('MEDV')),
        predict_fn=best_rf_model.predict,
        num_features=len(X_train.columns)
    )

    exp.as_pyplot_figure()
    plt.title(f"LIME Explanation for Instance {index} (Actual MEDV: {row['MEDV']})")
    plt.show()

"""**Reasoning**:
The previous attempts to generate LIME explanations still resulted in warnings.  I suspect the issue might be with how the data is being passed to the `predict_fn`, which expects a DataFrame with column names. I will attempt to reset the index of `df_lime` before passing it to the explainer.  I also noticed the previous code was not using the correct method to find the categorical features index.


"""

import lime
import lime.lime_tabular
import matplotlib.pyplot as plt
import numpy as np

# Reset index of df_lime to ensure correct alignment with X_train
df_lime = df_lime.reset_index(drop=True)


# Create a LimeTabularExplainer, ensuring feature names are consistent
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=None,
    categorical_features=[X_train.columns.get_loc('CHAS')], # Use get_loc to find the index
    mode='regression'
)

# Iterate through df_lime and generate explanations
for index, row in df_lime.iterrows():
    # Convert the row to a DataFrame with column names, matching X_train
    row_df = pd.DataFrame([row.drop('MEDV')], columns=X_train.columns)

    exp = explainer.explain_instance(
        data_row=np.array(row_df.iloc[0]),  # Access the first row of the DataFrame
        predict_fn=best_rf_model.predict,
        num_features=len(X_train.columns)
    )

    exp.as_pyplot_figure()
    plt.title(f"LIME Explanation for Instance {index} (Actual MEDV: {row['MEDV']})")
    plt.show()

!pip install shap

import shap

# ... (Your existing code) ...

# Calculate SHAP values
explainer = shap.TreeExplainer(best_rf_model)
shap_values = explainer.shap_values(X_test)

# Summarize the effects of all the features
shap.summary_plot(shap_values, X_test)

# Plot the SHAP values for a single prediction
# For example, for the first instance in the test set:
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])

import lime
import lime.lime_tabular
import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have already trained your 'best_rf_model' and have 'X_train', 'X_test'

# Choose an instance from the test set for comparison
instance_index = 0  # You can change this to any index in X_test
instance = X_test.iloc[instance_index]

# LIME Explanation
explainer_lime = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=None,
    categorical_features=[X_train.columns.get_loc('CHAS')],
    mode='regression'
)

exp_lime = explainer_lime.explain_instance(
    data_row=np.array(instance),
    predict_fn=best_rf_model.predict,
    num_features=len(X_train.columns)
)

# SHAP Explanation
explainer_shap = shap.TreeExplainer(best_rf_model)
shap_values = explainer_shap.shap_values(instance)

# Print and compare explanations
print("LIME Explanation:")
print(exp_lime.as_list())

print("\nSHAP Explanation:")
for i, feature in enumerate(X_train.columns):
    print(f"{feature}: {shap_values[i]}")

# Visualize LIME explanation
exp_lime.as_pyplot_figure()
plt.title("LIME Explanation")
plt.show()

# Visualize SHAP explanation
shap.force_plot(explainer_shap.expected_value, shap_values, instance, show=False, feature_names=X_train.columns)
plt.title("SHAP Explanation")
plt.tight_layout() # Adjust layout to prevent overlapping labels
plt.show()